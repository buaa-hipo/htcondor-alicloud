<HTML>
<HEAD>
<TITLE>condor_submit</TITLE>
<META NAME="description" CONTENT="condor_submit">
<META NAME="keywords" CONTENT="ref">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<LINK REL="STYLESHEET" HREF="ref.css">
<LINK REL="next" HREF="condor_submit_dag.html">
<LINK REL="previous" HREF="condor_status.html">
<LINK REL="up" HREF="8_Command_Reference.html">
<LINK REL="next" HREF="condor_submit_dag.html">
</HEAD>
<BODY  BGCOLOR=#FFFFFF >
<HR>

<H1><A NAME="SECTION009250000000000000000">&#160;</A><A NAME="man-condor-submit">&#160;</A>
<BR>
<I>condor_submit</I>
</H1> Queue jobs for execution on remote machines

<H2><A NAME="SECTION009251000000000000000">
Synopsis</A>
</H2><B><I>condor_submit</I></B>
[<B>-</B>]
[<B>-v</B>]
[<B>-n  </B><I>schedd_name</I>]
[<B>-r  </B><I>schedd_name</I>]
[<B>-d</B>]
<I>submit-description file</I>

<P>
<A NAME="23981">&#160;</A>
<A NAME="23982">&#160;</A>

<P>

<H2><A NAME="SECTION009252000000000000000">
Description</A>
</H2>

<P>
<I>condor_submit</I> is the program for submitting jobs to Condor.
<I>condor_submit</I> requires a submit-description file which contains commands
to direct the queuing of jobs. One description file may contain
specifications for the queuing of many condor jobs at once. All jobs queued by a
single invocation of <I>condor_submit</I> must share the same executable, and
are referred to as a ``job cluster''. It is advantageous to submit
multiple jobs as a single cluster because:
<UL>
<LI>Only one copy of the checkpoint file is needed to 
represent all jobs in a cluster until they begin execution.
<LI>There is much less overhead involved for Condor to start the next
job in a cluster than for Condor to start a new cluster.  This can make
a big difference if you are submitting lots of short running jobs.
</UL>
<P>
<EM>SUBMIT DESCRIPTION FILE COMMANDS</EM>

<P>
Each condor job description file describes one cluster of jobs to be
placed in the condor execution pool. All jobs in a cluster must share
the same executable, but they may have different input and output files,
and different program arguments, etc. The submit-description file is then
used as the only command-line argument to <I>condor_submit</I>. 

<P>
The submit-description file must contain one <I>executable</I> command and at least one
<I>queue</I> command.  All of the other commands have default actions.

<P>
The commands which can appear in the submit-description file are:

<P>
<DL>
<DD><P>
<DT><STRONG>executable = &lt;name&gt;</STRONG>
<DD>The name of the executable file for this
job cluster. Only one executable command may be present in a description
file. If submitting into the Standard Universe, which is the default,
then the named executable must have been re-linked with the Condor
libraries (such as via the <I>condor_compile</I> command). If submitting into
the Vanilla Universe, then the named executable need not be re-linked and
can be any process which can run in the background (shell scripts work
fine as well). 

<P>
<DT><STRONG>input = &lt;pathname&gt;</STRONG>
<DD>Condor assumes that its jobs are
long-running, and that the user will not wait at the terminal for their
completion. Because of this, the standard files which normally access
the terminal, (stdin, stdout, and stderr), must refer to files. Thus,
the filename specified with <B>input</B> should contain any keyboard
input the program requires (i.e. this file becomes stdin). If not
specified, the default value of /dev/null is used. 

<P>
<DT><STRONG>output = &lt;pathname&gt;</STRONG>
<DD>The <B>output</B> filename will capture
any information the program would normally write to the screen (i.e.
this file becomes stdout). If not specified, the default value of
/dev/null is used. More than one job should not use the same output
file, since this will cause one job to overwrite the output of
another.

<P>
<DT><STRONG>error = &lt;pathname&gt;</STRONG>
<DD>The <B>error</B> filename will capture any
error messages the program would normally write to the screen (i.e. this
file becomes stderr). If not specified, the default value of /dev/null
is used. More than one job should not use the same error file, since
this will cause one job to overwrite the errors of another.

<P>
<DT><STRONG>arguments = &lt;argument_list&gt;</STRONG>
<DD>List of arguments to be supplied
to the program on the command line. 

<P>
<DT><STRONG>initialdir = &lt;directory-path&gt;</STRONG>
<DD>Used to specify the current
working directory for the Condor job. Should be a path to a preexisting
directory. If not specified, <I>condor_submit</I> will automatically insert
the user's current working directory at the time <I>condor_submit</I> was run
as the value for <B>initialdir</B>. 

<P>
<DT><STRONG>requirements = &lt;ClassAd Boolean Expression&gt;</STRONG>
<DD>The requirements
command is a boolean ClassAd expression which uses C-like operators. In
order for any job in this cluster to run on a given machine, this
requirements expression must evaluate to true on the given machine. For
example, to require that whatever machine executes your program has a
least 64 Meg of RAM and has a MIPS performance rating greater than 45,
use: 
<PRE>
        requirements = Memory &gt;= 64 &amp;&amp; Mips &gt; 45
</PRE>Only one requirements command may be present in a
description file. By default, <I>condor_submit</I> 
appends the following clauses to the requirements expression:
<DL COMPACT>
<DT>1.
<DD>Arch and OpSys are set equal to the Arch and OpSys of the
submit machine.  In other words: unless you request otherwise, Condor will give your
job machines with the same architecture and operating system version as
the machine running <I>condor_submit</I>.
<DT>2.
<DD>Disk &gt; ExecutableSize.  To ensure there is enough disk space on the 
target machine for Condor to copy over your executable.
	<DT>3.
<DD>VirtualMemory &gt;= ImageSize.  To ensure the target machine
has enough virtual memory to run your job.
	<DT>4.
<DD>If Universe is set to Vanilla, FileSystemDomain is set equal to
the submit machine's FileSystemDomain.
</DL>You can view the requirements of a job
which has already been submitted (along with everything else about the
job ClassAd) with the command <I>condor_q -l</I>; see the command reference for
<I>condor_q</I> on page&nbsp;<A HREF="condor_q.html#man-condor-q"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]" SRC="cross_ref_motif.gif"></A>.  Also, see the Condor Users
Manual for complete information on the syntax and available attributes
that can be used in the ClassAd expression.

<P>
<DT><STRONG>rank = &lt;ClassAd Float Expression&gt;</STRONG>
<DD>A ClassAd Floating-Point 
expression that states how to rank machines which have already met the requirements
expression. Essentially, rank expresses preference.  A higher numeric value 
equals better rank. Condor will give the job the machine with the 
highest rank.  For example,
<PRE>
        requirements = Memory &gt; 60
        rank = Memory
</PRE>asks Condor to find all available machines with more than 60 megabytes of memory
and give the job the one with the most amount of memory.  See the Condor Users
Manual for complete information on the syntax and available attributes
that can be used in the ClassAd expression.

<P>
<DT><STRONG>on_exit_remove = &lt;ClassAd Boolean Expression&gt;</STRONG>
<DD>This expression
is checked when the job exits and if true, then it allows the job to leave the
queue normally. If false, then the job is placed back into the Idle state.
If the user job is a vanilla job then it restarts from the beginning. If the
user job is a standard job, then it restarts from the last checkpoint.

<P>
For example:
Suppose you have a job that occasionally segfaults but you know if you run it
again on the same data, chances are it will finish successfully. This is
how you would represent that with <TT>on_exit_remove</TT>(assuming the
signal identifier for segmentation fault is 4):

<P>
<PRE>
	on_exit_remove = (ExitBySignal == True) &amp;&amp; (ExitSignal != 4)
</PRE>
<P>
The above expression will not let the job exit if it exited by a signal and
that signal number was 4(representing segmentaion fault). In any other case
of the job exiting, it will leave the queue as it normally would have done.

<P>
If left unspecified, this will default to <TT>True</TT>.

<P>
periodic_* expressions(defined elsewhere in this man page) take
precedent over on_exit_* expressions and a *_hold expression takes
precedent over a *_remove expression.

<P>
This expression is available only under UNIX and only for the standard and 
vanilla universes.

<P>
<DT><STRONG>on_exit_hold = &lt;ClassAd Boolean Expression&gt;</STRONG>
<DD>This expression
is checked when the job exits and if true, places the job on hold. If false
then nothing happens and the <TT>on_exit_remove</TT> expression is
checked to determine if that needs to be applied.

<P>
For example:
Suppose you have a job that you know will run for an hour minimum. If
the job exits after less than an hour, you would like it to be placed on
hold and notified by e-mail instead of being allowed to leave the queue.

<P>
<PRE>
	on_exit_hold = (ServerStartTime - JobStartDate) &lt; 3600
</PRE>
<P>
The above expression will place the job on hold if it exits for any reason
before running for an hour. An e-mail will be sent to the user explaining
that the job was placed on hold because this expression became true.

<P>
periodic_* expressions(defined elsewhere in this man page) take
precedent over on_exit_* expressions and any *_hold expression takes
precedent over a *_remove expression.

<P>
If left unspecified, this will default to <TT>False</TT>.

<P>
This expression is available only under UNIX and only for the standard and 
vanilla universes.

<P>
<DT><STRONG>periodic_remove = &lt;ClassAd Boolean Expression&gt;</STRONG>
<DD>This expression is checked every 20 seconds(non-configurable,
but might be in future) and if it becomes true, the job will
leave the queue. <TT>periodic_remove</TT> takes precedent over
<TT>on_exit_remove</TT> if the two describe conflicting states.

<P>
For example:
Suppose you would like your job removed if the total suspension time of the
job is more than half of the run time of the job.

<P>
<PRE>
	periodic_remove = CumulativeSuspensionTime &gt; (RemoteWallClockTime / 2.0)
</PRE>
<P>
The above expression will remove the job once the conditions have become true.

<P>
<B>Notice: </B>
<BR> 
<!-- MATH: $\fbox{\parbox[t]{\textwidth}{Currently, this option will force a ``terminate'' event in the user
log of the job and it will report a successful termination of the job. This
reporting of this event will be changed in a future version of Condor to
be a ``job aborted event'' with descriptions of who intitated the event. As
it stands now, someone looking at the user log termination event wouldn't be
able to tell the difference between a job that legitimately ended successfully
and a job where the \texttt{periodic\_remove} expression had become true.}}$ -->
<IMG
 WIDTH="567" HEIGHT="140" ALIGN="BOTTOM" BORDER="0"
 SRC="img24.gif"
 ALT="\fbox{\parbox[t]{\textwidth}{Currently, this option will force a \lq\lq terminate'' e...
...ully
and a job where the \texttt{periodic\_remove} expression had become true.}}">

<P>
periodic_* expressions(defined elsewhere in this man page) take
precedent over on_exit_* expressions and any *_hold expression takes
precedent over a *_remove expression.

<P>
If left unspecified, this will default to <TT>False</TT>.

<P>
This expression is available only under UNIX and only for the standard and 
vanilla universes.

<P>
<DT><STRONG>periodic_hold = &lt;ClassAd Boolean Expression&gt;</STRONG>
<DD>This expression
is checked every 20 seconds(non-configurable, but might be in future) and
if it becomes true, the job will be placed on hold.

<P>
For example:
Suppose you would like your job held if the total suspension time of the
job is more than half of the total run time of the job.

<P>
<PRE>
	periodic_hold = CumulativeSuspensionTime &gt; (RemoteWallClockTime / 2.0)
</PRE>
<P>
The above expression will place the job on hold if it suspends longer
than half the amount of time it has totally run.  An e-mail will be
sent to the user explaining that the job was placed on hold because this
expression became true.

<P>
If left unspecified, this will default to <TT>False</TT>.

<P>
periodic_* expressions(defined elsewhere in this man page) take
precedent over on_exit_* expressions and any *_hold expression takes
precedent over a *_remove expression.

<P>
This expression is available only under UNIX and only for the standard and 
vanilla universes.

<P>
<DT><STRONG>priority = &lt;priority&gt;</STRONG>
<DD>Condor job priorities range from -20 to
+20, with 0 being the default. Jobs with higher numerical priority will
run before jobs with lower numerical priority. Note that this priority
is on a per user basis; setting the priority will determine the order in
which your own jobs are executed, but will have no effect on whether or
not your jobs will run ahead of another user's jobs. 

<P>
<DT><STRONG>notification = &lt;when&gt;</STRONG>
<DD><A NAME="man-condor-submit-notification">&#160;</A> Owners of condor jobs are notified by
email when certain events occur.
If <I>when</I> is set to Always, the owner will be notified
whenever the job is checkpointed, and when it completes.
If <I>when</I> is set to Complete (the default), the owner will
be notified when the job terminates.
If <I>when</I> is set to Error, the owner will only be notified
if the job terminates abnormally.
If <I>when</I> is set to Never, the owner will not be mailed,
regardless what happens to the job.
The statistics included in the email are documented in
section&nbsp;<A HREF="2_6Managing_Condor.html#sec:job-completion">2.6.5</A> on
page&nbsp;<A HREF="2_6Managing_Condor.html#sec:job-completion"><IMG  ALIGN="BOTTOM" BORDER="1" ALT="[*]" SRC="cross_ref_motif.gif"></A>.

<P>
<DT><STRONG>notify_user = &lt;email-address&gt;</STRONG>
<DD><A NAME="man-condor-submit-notify-user">&#160;</A> Used to specify the email
address to use when Condor sends email about a job.  If not specified,
Condor will default to using :
<PRE>
        job-owner@UID_DOMAIN
</PRE>where <TT>UID_DOMAIN</TT> <A NAME="24334">&#160;</A> <A NAME="24335">&#160;</A> is specified by the Condor site administrator.  If 
<TT>UID_DOMAIN</TT> <A NAME="24339">&#160;</A> <A NAME="24340">&#160;</A> has not been specified, Condor will send the email
to :
<PRE>
        job-owner@submit-machine-name
</PRE>
<P>
<DT><STRONG>copy_to_spool = &lt;True |  False&gt;</STRONG>
<DD>If <B>copy_to_spool</B> is set to
<I>True</I>, then <I>condor_submit</I> will copy the executable to the local spool 
directory before running it on a remote host. Oftentimes this can be quite
time consuming and unnecessary. By setting it to <I>False</I>, <I>condor_submit</I>
will skip this step.  Defaults to <I>True</I>.

<P>
<DT><STRONG>getenv = &lt;True |  False&gt;</STRONG>
<DD>If <B>getenv</B> is set to
<I>True</I>, then <I>condor_submit</I> will copy all of the user's current
shell environment variables at the time of job submission into the job
ClassAd. The job will therefore execute with the same set of environment
variables that the user had at submit time. Defaults to <I>False</I>.
You must be careful when using this feature, since the maximum allowed
size of the environment in Condor is 10240 characters.  
If your environment is larger than that, Condor will not allow you to
submit your job, and you will have to use the ``Environment'' setting
described below, instead.

<P>
<DT><STRONG>hold = &lt;True |  False&gt;</STRONG>
<DD>If <B>hold</B> is set to
<I>True</I>, then the job will be submitted in the hold state.  Jobs in
the hold state will not run until released by <I>condor_release</I>.

<P>
<DT><STRONG>environment = &lt;parameter_list&gt;</STRONG>
<DD>List of environment variables
of the form :
<PRE>
        &lt;parameter&gt; = &lt;value&gt;
</PRE>Multiple environment variables can be specified by separating them with a
semicolon (`` ; ''). These environment variables will be placed into the
job's environment before execution. The length of all characters
specified in the environment is currently limited to 10240 characters.  

<P>
<DT><STRONG>log = &lt;pathname&gt;</STRONG>
<DD>Use <B>log</B> to specify a filename where
Condor will write a log file of what is happening with this job cluster.
For example, Condor will log into this file when and where the job
begins running, when the job is checkpointed and/or migrated, when the
job completes, etc. Most users find specifying a <B>log</B> file to be very
handy; its use is recommended. If no <B>log</B> entry is specified, 
Condor does not create a log for this cluster.

<P>
<DT><STRONG>universe = &lt;vanilla |  standard |  pvm |  scheduler
|  globus |  mpi&gt;</STRONG>
<DD>Specifies which Condor Universe to use when running this job.  The Condor 
Universe specifies a Condor execution environment.  The <I>standard</I> 
Universe is the default, and tells Condor that this job has been re-linked 
via <I>condor_compile</I> with the Condor libraries and therefore supports
checkpointing and remote system calls.  The <I>vanilla</I> Universe is an
execution environment for jobs which have not been linked with the
Condor libraries.  <I>Note:</I> use the <I>vanilla</I> Universe to
submit shell scripts to Condor.  The <I>pvm</I> Universe is for a
parallel job written with PVM 3.4. The <I>scheduler</I> is for a job that
should act as a metascheduler.
The <I>globus</I> universe uses the Globus
GRAM API to contact the Globus resource specifed and requests it run the job.
The <I>mpi</I> universe is
for running mpi jobs made with the MPICH package.
See the Condor User's Manual for more information about using Universe.

<P>
<DT><STRONG>image_size = &lt;size&gt;</STRONG>
<DD>This command tells Condor the maximum
virtual image size to which you believe your program will grow during
its execution. Condor will then execute your job only on machines which
have enough resources, (such as virtual memory), to support executing
your job. If you do not specify the image size of your job in the
description file, Condor will automatically make a (reasonably accurate)
estimate about its size and adjust this estimate as your program runs.
If the image size of your job is underestimated, it may crash due to
inability to acquire more address space, e.g. malloc() fails. If the image
size is overestimated, Condor may have difficulty finding machines which
have the required resources. <I>size</I> must be in kbytes, e.g. for
an image size of 8 megabytes, use a <I>size</I> of 8000.

<P>
<DT><STRONG>machine_count = &lt;min..max&gt; |  &lt;max&gt;</STRONG>
<DD>If <B>machine_count</B> is
specified, Condor will not start the job until it can simultaneously
supply the job with <I>min</I> machines.  Condor will continue to try 
to provide up
to <I>max</I> machines, but will not delay starting of the job to do so.
If the job is started with fewer than <I>max</I> machines, the job
will be notified via a usual PvmHostAdd notification as additional
hosts come on line.
<B>Important:</B> only use <B>machine_count</B> if an only if
submitting into the PVM or MPI Universes.  Use min..max for the PVM
universe, and just max for the MPI universe.

<P>
<DT><STRONG>coresize = &lt;size&gt;</STRONG>
<DD>Should the user's program abort and produce
a core file, <B>coresize</B> specifies the maximum size in bytes of the
core file which the user wishes to keep. If <B>coresize</B> is not
specified in the command file, the system's user resource limit
``coredumpsize'' is used (except on HP-UX). 

<P>
<DT><STRONG>nice_user = &lt;True |  False&gt;</STRONG>
<DD><A NAME="man-condor-submit-nice">&#160;</A>Normally, when a machine
becomes available to Condor, Condor decides which job to run based upon
user and job priorities. Setting <B>nice_user</B> equal to <I>True</I>
tells Condor not to use your regular user priority, but that this job
should have last priority amongst all users and all jobs. So jobs
submitted in this fashion run only on machines which no other
non-nice_user job wants -- a true ``bottom-feeder'' job! This is very
handy if a user has some jobs they wish to run, but do not wish to use
resources that could instead be used to run other people's Condor jobs. Jobs
submitted in this fashion have ``nice-user.'' pre-appended in front of
the owner name when viewed from <I>condor_q</I> or <I>condor_userprio</I>.  The
default value is <B>False</B>.

<P>
<DT><STRONG>kill_sig = &lt;signal-number&gt;</STRONG>
<DD>When Condor needs to kick a job
off of a machine, it will send the job the signal specified by
<I>signal-number</I>.  <I>signal-number</I> needs to be an integer which
represents a valid signal on the execution machine.  For jobs submitted
to the Standard Universe, the default value is the number for
<code>SIGTSTP</code> which tells the Condor libraries to initiate a checkpoint
of the process.  For jobs submitted to the Vanilla Universe, the default 
is <code>SIGTERM</code> which is the standard way to terminate a program in UNIX.  

<P>
<DT><STRONG>compress_files = file1, file2, ...</STRONG>
<DD><P>
If your job attempts to access any of the files mentioned in this list,
Condor will automatically compress them (if writing) or decompress them (if reading).
The compress format is the same as used by GNU gzip.

<P>
The files given in this list may be simple filenames or complete paths and may
include * as a wildcard.  For example, this list causes the file /tmp/data.gz,
any file named event.gz, and any file ending in .gzip to be automatically
compressed or decompressed as needed:

<P>
<PRE>
compress_files = /tmp/data.gz, event.gz, *.gzip
</PRE>
<P>
Due to the nature of the compression format, compressed files must only
be accessed sequentially.  Random access reading is allowed but is very slow,
while random access writing is simply not possible.  This restriction may be
avoided by using both compress_files and fetch_files at the same time.  When
this is done, a file is kept in the decompressed state at the execution
machine, but is compressed for transfer to its original location.

<P>
This option only applies to standard-universe jobs.

<P>
<DT><STRONG>fetch_files = file1, file2, ...</STRONG>
<DD><P>
If your job attempts to access a file mentioned in this list,
Condor will automatically copy the whole file to the executing machine,
where it can be accessed quickly.  When your job closes the file,
it will be copied back to its original location.
This list uses the same syntax as compress_files, shown above.

<P>
This option only applies to standard-universe jobs.

<P>
<DT><STRONG>append_files = file1, file2, ...</STRONG>
<DD><P>
If your job attempts to access a file mentioned in this list,
Condor will force all writes to that file to be appended to the end.
Furthermore, condor_submit will not truncate it.
This list uses the same syntax as compress_files, shown above.

<P>
This option may yield some surprising results.  If several
jobs attempt to write to the same file, their output may be intermixed.
If a job is evicted from one or more machines during the course of its
lifetime, such an output file might contain several copies of the results.
This option should be only be used when you wish a certain file to be
treated as a running log instead of a precise result.

<P>
This option only applies to standard-universe jobs.

<P>
<DT><STRONG>local_files = file1, file2, ...</STRONG>
<DD><P>
If your job attempts to access a file mentioned in this list,
Condor will cause it to be read or written at the execution machine.
This is most useful for temporary files not used for input or output.
This list uses the same syntax as compress_files, shown above.

<P>
<PRE>
local_files = /tmp/*
</PRE>
<P>
This option only applies to standard-universe jobs.

<P>
<DT><STRONG>file_remaps = &lt; `` name = newname ; name2 = newname2 ... ''&gt; </STRONG>
<DD><P>
Directs Condor to use a new filename in place of an old one.  <I>name</I>
describes a filename that your job may attempt to open, and <I>newname</I>
describes the filename it should be replaced with.
<I>newname</I> may include an optional leading
access specifier, <code>local:</code> or <code>remote:</code>.  If left unspecified,
the default access specifier is <code>remote:</code>.  Multiple remaps can be 
specified by separating each with a semicolon.

<P>
This option only applies to standard-universe jobs.

<P>
If you wish to remap file names that contain equals signs or semicolons,
these special chracaters may be escaped with a backslash.

<P>
This option only applies to standard-universe jobs.

<P>
<DL>
<DT><STRONG>Example One:</STRONG>
<DD>Suppose that your job reads a file named <code>dataset.1</code>.  To instruct Condor
to force your job to read <code>other.dataset</code> instead, 
add this to the submit file:
<PRE>
file_remaps = "dataset.1=other.dataset"
</PRE><DT><STRONG>Example Two:</STRONG>
<DD>Suppose that your run many jobs which all read in the same large file,
called <code>very.big</code>.  If this file can be found in the same place on
a local disk in every machine in the pool,
(say <code>/bigdisk/bigfile</code>,) you can
instruct Condor of this fact by remapping <code>very.big</code> to
<code>/bigdisk/bigfile</code> and specifying that the file is to be read locally,
which will be much faster than reading over the network.
<PRE>
file_remaps = "very.big = local:/bigdisk/bigfile"
</PRE><DT><STRONG>Example Three:</STRONG>
<DD>Several remaps can be applied at once by separating each with a semicolon.
<PRE>
file_remaps = "very.big = local:/bigdisk/bigfile ; dataset.1 = other.dataset"
</PRE></DL>
<P>
<DT><STRONG>buffer_files = &lt; `` name = (size,block-size) ; name2 = (size,block-size) ... '' &gt; </STRONG>
<DD><DT><STRONG>buffer_size = &lt;bytes-in-buffer&gt;</STRONG>
<DD><DT><STRONG>buffer_block_size = &lt;bytes-in-block&gt;</STRONG>
<DD>Condor keeps a buffer of recently-used data for each file a job accesses.
This buffer is used both to cache commonly-used data and to consolidate small
reads and writes into larger operations that get better throughput.
The default settings should produce reasonable results for most programs.

<P>
These options only apply to standard-universe jobs.

<P>
If needed, you may set the buffer controls individually for each file using
the buffer_files option. For example, to set the buffer size to 1MB and
the block size to 256KB for the file 'input.data', use this command:

<P>
<PRE>
buffer_files = "input.data=(1000000,256000)"
</PRE>
<P>
Alternatively, you may use these two options to set
the default sizes for all files used by your job:

<P>
<PRE>
buffer_size = 1000000
buffer_block_size = 256000
</PRE>
<P>
If you do not set these, Condor will use the values given by these
two config file macros:

<P>
<PRE>
DEFAULT_IO_BUFFER_SIZE = 1000000
DEFAULT_IO_BUFFER_BLOCK_SIZE = 256000
</PRE>
<P>
Finally, if no other settings are present, Condor will use a buffer of 512KB
and a block size of 32KB.

<P>
<DT><STRONG>rendezvousdir = &lt;directory-path&gt;</STRONG>
<DD>Used to specify the 
shared-filesystem directory to be used for filesystem authentication
when submitting to a remote scheduler.  Should be a path to a preexisting 
directory.  

<P>
<DT><STRONG>x509directory = &lt;directory-path&gt;</STRONG>
<DD>Used to specify the directory 
which contains the certificate, private key, and trusted certificate directory
for GSS authentication. If this attribute is set, the environment variables 
X509_USER_KEY, X509_USER_CERT, and X509_CERT_DIR are exported with 
default values. See section&nbsp;<A HREF="3_9Using_X_509.html#sec:X509-Authentication">3.9</A> for more info.

<P>
<DT><STRONG>x509userproxy = &lt;full-pathname&gt;</STRONG>
<DD>Used to override the default
pathname for X509 user certificates. The default location for X509 proxies
is the /tmp directory, which is generally a local filesystem. Setting
this value would allow Condor to access the proxy in a shared filesystem
(e.g., AFS). Condor will use the proxy specified in the submit file
first. If nothing is specified in the submit file, it will use the
environment variable X509_USER_CERT. If that variable is not present,
it will search in the default location. See 
section&nbsp;<A HREF="3_9Using_X_509.html#sec:X509-Authentication">3.9</A> for more info.

<P>
<DT><STRONG>globusscheduler = &lt;scheduler-name&gt;</STRONG>
<DD>Used to specify the 
Globus resource to which the job should be submitted. More than one scheduler
can be submitted to, simply place a <I>queue</I> command after each instance
of globusscheduler. Each instance should be a valid Globus scheduler, using
either the full Globus contact string or the host/scheduler format shown below:
<DL>
<DT><STRONG>Example:</STRONG>
<DD>To submit to the LSF scheduler of the Globus gatekeeper on lego at 
Boston University:
<PRE>
...
GlobusScheduler = lego.bu.edu/jobmanager-lsf
queue
</PRE></DL>
<P>
<DT><STRONG>globus_rsl = &lt;RSL-string&gt;</STRONG>
<DD>Used to provide any additional Globus RSL
string attributes which are not covered by regular submit file parameters.

<P>
<DT><STRONG>transfer_executable = &lt;True |  False&gt;</STRONG>
<DD>If <B>transfer_executable</B> is set to
<I>false</I>, then Condor look for the executable on the remote machine, and
not transfer it over. This is useful if you have already pre-staged your
executable, and wish to have Condor behave more like rsh. Defaults to <I>True</I>.
This option is only used in the Globus universe.

<P>
<DT><STRONG>+&lt;attribute&gt; = &lt;value&gt;</STRONG>
<DD>A line which begins with a '+'
(plus) character instructs <I>condor_submit</I> to simply insert the
following <I>attribute</I> into the job ClasssAd with the given 
<I>value</I>. 

<P>
<DT><STRONG>queue [number-of-procs</STRONG>
<DD>] Places one or more copies of the job into
the Condor queue. If desired, new <B>input</B>, <B>output</B>,
<B>error</B>, <B>initialdir</B>, <B>arguments</B>, <B>nice_user</B>,
<B>priority</B>, <B>kill_sig</B>, <B>coresize</B>, or <B>image_size</B>
commands may be issued between <B>queue</B> commands. This is very handy
when submitting multiple runs into one cluster with one submit file; for
example, by issuing an <B>initialdir</B> between each <B>queue</B>
command, each run can work in its own subdirectory. The optional
argument <I>number-of-procs</I> specifies how many times to submit the
job to the queue, and defaults to 1.

<P>
</DL>
<P>
In addition to commands, the submit-description file can contain macros
and comments:

<P>
<DL>
<DD><P>
<DT><STRONG>Macros</STRONG>
<DD>Parameterless macros in the form of <TT>$(macro_name)</TT><A NAME="24464">&#160;</A><A NAME="24465">&#160;</A>
may be inserted anywhere in condor description files. Macros can be
defined by lines in the form of 
<PRE>
 
        &lt;macro_name&gt; = &lt;string&gt;
</PRE> 
Two pre-defined macros are supplied by the description file parser. The
<TT>$(Cluster)</TT><A NAME="24469">&#160;</A><A NAME="24470">&#160;</A> macro supplies the number of the job cluster, and the
<TT>$(Process)</TT><A NAME="24474">&#160;</A><A NAME="24475">&#160;</A> macro supplies the number of the job. These macros are
intended to aid in the specification of input/output files, arguments,
etc., for clusters with lots of jobs, and/or could be used to supply a
Condor process with its own cluster and process numbers on the command
line.  The <TT>$(Process)</TT><A NAME="24479">&#160;</A><A NAME="24480">&#160;</A> macro should not be used for PVM jobs.

<P>
If you happen to want a ``$'' as a literal character, then you must use
<PRE>
$(DOLLAR)
</PRE>
<P>
In addition to the normal macro, there is also a special kind of macro
called a ``<EM>Substitution Macro</EM>'' that allows you to substitue
expressions defined on the resource machine itself(gotten after a match
to the machine has been performed) into specific expressions in your
submit description file. The special substitution macro is of the form:
<PRE>
 
$$(attribute)
</PRE>
<P>
The substitution macro can only be used in three expressions in the
submit description file: <TT>executable</TT> <A NAME="24484">&#160;</A> <A NAME="24485">&#160;</A>, <TT>environment</TT> <A NAME="24489">&#160;</A> <A NAME="24490">&#160;</A>, and
<TT>arguments</TT> <A NAME="24494">&#160;</A> <A NAME="24495">&#160;</A>. The most common use of this macro is for heterogeneous
submission of an executable:
<PRE>
executable = povray.$$(opsys).$$(arch)
</PRE>The <TT>opsys</TT> and <TT>arch</TT> attributes will be substituted at
match time for any given resource. This will allow Condor to automatically
choose the right executable for the right machine.

<P>
<DT><STRONG>Comments</STRONG>
<DD>Blank lines and lines beginning with a '#' (pound-sign)
character are ignored by the submit-description file parser. 

<P>
</DL>
<P>

<H2><A NAME="SECTION009253000000000000000">
Options</A>
</H2>
  Supported options are as follows:
  
<P>
  <DL>
<DD><P>
<DT><STRONG><B>-</B></STRONG>
<DD>Accept the command file from stdin.
<BR>
    <DT><STRONG><B>-v</B></STRONG>
<DD>Verbose output - display the created job class-ad
<BR>

<P>
<DT><STRONG><B>-n </B><I>schedd_name</I></STRONG>
<DD>Submit to the specified schedd. This option is used when there is more than one schedd running on the submitting machine
<BR>

<P>
<DT><STRONG><B>-r </B><I>schedd_name</I></STRONG>
<DD>Submit to a remote schedd. The jobs
	will be submitted to the schedd on the specified remote host. On Unix
	systems, the Condor administrator for you site must override the default 
	AUTHENTICATION_METHODS configuration setting to enable remote filesystem 
	(FS_REMOTE) authentication.
<BR>

<P>
<DT><STRONG><B>-d</B></STRONG>
<DD>Disable file permission checks.
<BR>

<P>
<DT><STRONG><B>-a </B><I>command</I></STRONG>
<DD>Augment the commands in the submit
    file with the given command. This command will be considered to
    immediately precede the Queue command in the submit file and come
    after all other previous commands. The submit file is not
    modified. You can append multiple commands by using the -a option
    multiple times. If your command has spaces in it, make sure you
    quote it.
<BR>

<P>
</DL>  
<P>

<P>

<H2><A NAME="SECTION009254000000000000000">
Exit Status</A>
</H2>

<P>
<I>condor_submit</I> will exit with a status value of 0 (zero) upon success, and a
non-zero value upon failure.

<P>

<H2><A NAME="SECTION009255000000000000000">
Examples</A>
</H2>

<P>
<U>Example 1</U>: The below example queues three jobs for
execution by Condor. The first will be given command line arguments of
'15' and '2000', and will write its standard output to 'foo.out1'. The
second will be given command line arguments of '30' and '2000', and will
write its standard output to 'foo.out2'. Similarly the third will have
arguments of '45' and '6000', and will use 'foo.out3' for its standard
output. Standard error output, (if any), from all three programs will
appear in 'foo.error'.

<P>
<PRE>
      ####################
      #
      # Example 1: queueing multiple jobs with differing
      # command line arguments and output files.
      #                                                                      
      ####################                                                   
                                                                         
      Executable     = foo                                                   
                                                                         
      Arguments      = 15 2000                                               
      Output  = foo.out1                                                     
      Error   = foo.err1
      Queue                                                                  
                                                                         
      Arguments      = 30 2000                                               
      Output  = foo.out2                                                     
      Error   = foo.err2
      Queue                                                                  
                                                                         
      Arguments      = 45 6000                                               
      Output  = foo.out3                                                     
      Error   = foo.err3
      Queue
</PRE>
<P>
<U>Example 2</U>: This submit-description file example queues 150
runs of program 'foo' which must have been compiled and linked for
Silicon Graphics workstations running IRIX 6.x. Condor will not attempt
to run the processes on machines which have less than 32 megabytes of
physical memory, and will run them on machines which have at least 64
megabytes if such machines are available. Stdin, stdout, and stderr will
refer to ``in.0'', ``out.0'', and ``err.0'' for the first run of this program
(process 0). Stdin, stdout, and stderr will refer to ``in.1'', ``out.1'',
and ``err.1'' for process 1, and so forth. A log file containing entries
about where/when Condor runs, checkpoints, and migrates processes in this
cluster will be written into file ``foo.log''.

<P>
<PRE>
      ####################                                                    
      #                                                                       
      # Example 2: Show off some fancy features including                            
      # use of pre-defined macros and logging.                                
      #                                                                       
      ####################                                                    
                                                                          
      Executable     = foo                                                    
      Requirements   = Memory &gt;= 32 &amp;&amp; OpSys == "IRIX6" &amp;&amp; Arch =="SGI"     
      Rank           = Memory &gt;= 64
      Image_Size     = 28 Meg                                                 
                                                                          
      Error   = err.$(Process)                                                
      Input   = in.$(Process)                                                 
      Output  = out.$(Process)                                                
      Log = foo.log                                                                       
                                                                          
      Queue 150
</PRE>
<P>

<H2><A NAME="SECTION009256000000000000000">
General Remarks</A>
</H2>
<UL>
<P>
<LI>For security reasons, Condor will refuse to run any jobs submitted
by user root (UID = 0) or by a user whose default group is group wheel
(GID = 0). Jobs submitted by user root or a user with a default group of
wheel will appear to sit forever in the queue in an idle state. 

<P>
<LI>All pathnames specified in the submit-description file must be
less than 256 characters in length, and command line arguments must be
less than 4096 characters in length; otherwise, <I>condor_submit</I> gives a
warning message but the jobs will not execute properly. 

<P>
<LI>Somewhat understandably, behavior gets bizzare if the user makes
the mistake of requesting multiple Condor jobs to write to the
same file, and/or if the user alters any files that need to be accessed
by a Condor job which is still in the queue (i.e. compressing of data or
output files before a Condor job has completed is a common mistake).

<P>
<LI>To disable checkpointing for Standard Universe jobs, include the
line:
<PRE>
      +WantCheckpoint = False
</PRE>in the submit-description file before the queue command(s).
</UL>
<P>

<H2><A NAME="SECTION009257000000000000000">
See Also</A>
</H2>
Condor User Manual

<P>

<H2><A NAME="SECTION009258000000000000000">
Author</A>
</H2> Condor Team, University of Wisconsin-Madison
  
<H2><A NAME="SECTION009259000000000000000">
Copyright</A>
</H2> Copyright &#169; 1990-2007 HTCondor Team, Computer Sciences Department,
  University of Wisconsin-Madison, Madison, WI.

  This source code is covered by the Apache License, Version 2.0, which
  can be found in the accompanying LICENSE-2.0.txt file, or online at
  http://www.apache.org/licenses/ .

  This product includes software developed by and/or derived from the
  Globus Project (http://www.globus.org/) to which the U.S. Government
  retains certain rights. Copyright (c) 1999 University of Chicago and
  The University of Southern California. All Rights Reserved.

  Some distributions of Condor include software developed by the
  Info-ZIP Project (http://www.info-zip.org/).  Complete conditions
  and disclaimers for Info-ZIP can be found at
  http://www.info-zip.org/doc/LICENSE

  Some distributions of Condor include MAKEMSI software developed by
  Dennis Bareis (http://dennisbareis.com/makemsi.htm).  Complete
  conditions and disclaimers for MAKEMSI can be found at
  http://makemsi-manual.dennisbareis.com/disclaimer.htm

  Some distributions of Condor include a compiled, unmodified version
  of the GNU C library. The complete source code to GNU glibc can be
  found at http://www.gnu.org/software/libc/.

  Part of the software embedded in this product is gSOAP software.
  Portions created by gSOAP are Copyright (C) 2001-2004 Robert A. van
  Engelen, Genivia inc. All Rights Reserved.
  THE SOFTWARE IN THIS PRODUCT WAS IN PART PROVIDED BY GENIVIA INC AND
  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
  GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER
  IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
  OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
<P>
See the <EM>HTCondor Manual</EM> for additional notices.
<P>

  <HR>
<ADDRESS>
condor-admin@cs.wisc.edu
</ADDRESS>
</BODY>
</HTML>
